####一：BLSTM
1. 常使用于序列相关的语序处理中，而程序也是一种语序，利用BLSTM能够从前向和后向序列中得到整个程序的信息。
2. 一个程序的漏洞往往处于一定的环境中，因此BLSTM十分适合该问题。
####二：词向量
1. 对程序进行处理，必须将程序转为向量，然后才能用于深度学习。
2. 利用keras.preprocessing.text.text_to_word_sequence实现分词，这里舍弃掉一部分符号和括号等分词。
3. 利用genism包的word2vector对所有分词生成词向量。
4. 遇到没有见过的词，生成为空下标[]
5. 利用pad_sequence()对样本进行填前向填充。使用的长度为100词。

问题：
###一：有哪些难点，如何解决
####1. 采用何种算法与模型
1. 普通机器学习算法高度依赖于特征的选择，因此采用深度学习算法。
2. 程序也可以看做一种语言，因此可以将其当做NLP问题处理，常用方法为RNN，LSTM，BLSTM等
3. 根据程序结构来看，采用BLSTM模型效果最好
####2.

###二：embedding的实现过程，原理是什么
####1. 分词
1. 使用keras.preprocessing.text.tokenizer对输入的数据进行分词，并通过**下标序列**表示输入数据
2. 使用keras.preprocessing.text.text_to_word_sequence分词,得到数据集样本所有的分词
####2. 生成词向量
1. 使用genism包的word2vector将得到的样本集所有分词生成对应的词向量(150维)
####3. embedding层
1. 在BLSTM的第一层加入embedding层，负责将**下标序列表示的输入数据**转为向量模式并传入BLSTM层
###三：word2vec的原理时什么
1. Distributed representation，能够描述词与词之间的距离
2. 本质是两层神经网络，用输入词预测临近的词
###四：碰到之前没有遇见过的字符串是怎么处理的
1. 直接输出[]
###五：为什么使用BLSTM， 相比LSTM模型的优点在哪，BLSTM的原理细节，包括画图，门等，数学公式
1. LSTM常使用于序列相关的语序处理中，而程序也是一种语序，利用BLSTM能够从前向和后向序列中得到整个程序的信息,效果较LSTM要好。
2. 一个程序的漏洞往往处于一定的上下文环境中，因此BLSTM十分适合该问题
3.